\documentclass[reprint,english,notitlepage]{revtex4-1}  % defines the basic parameters of the document

% if you want a single-column, remove reprint

% allows special characters (including æøå)
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

%% note that you may need to download some of these packages manually, it depends on your setup.
%% I recommend downloading TeXMaker, because it includes a large library of the most common packages.

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
\usepackage{graphicx}         % include graphics such as plots
\usepackage{xcolor}           % set colors
\usepackage{hyperref}         % automagic cross-referencing (this is GODLIKE)
\usepackage{tikz}             % draw figures manually
\usepackage{listings}         % display code
\usepackage{subfigure}        % imports a lot of cool and useful figure commands

% defines the color of hyperref objects
% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ % this is just my personal choice, feel free to change things
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}
    
%% Defines the style of the programming listing
%% This is actually my personal template, go ahead and change stuff if you want
\lstnewenvironment{python}{
	\lstset{ %
		inputpath=,
		backgroundcolor=\color{white!88!black},
		basicstyle={\ttfamily\scriptsize},
		commentstyle=\color{magenta},
		language=Python,
		morekeywords={True,False},
		tabsize=4,
		stringstyle=\color{green!55!black},
		frame=single,
		keywordstyle=\color{blue},
		showstringspaces=false,
		columns=fullflexible,
		keepspaces=true}
}{}

\lstnewenvironment{cpp}{
	\lstset{ %
		inputpath=,
		backgroundcolor=\color{white!88!black},
		basicstyle={\ttfamily\scriptsize},
		commentstyle=\color{magenta},
		language=C++,
		morekeywords={True,False},
		tabsize=4,
		stringstyle=\color{green!55!black},
		frame=single,
		keywordstyle=\color{blue},
		showstringspaces=false,
		columns=fullflexible,
		keepspaces=true}
}{}

\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}



\usepackage{thmtools}
\DeclareMathOperator{\nullspace}{Nul}
\DeclareMathOperator{\collspace}{Col}
\DeclareMathOperator{\rref}{Rref}
%%\DeclareMathOperator{\dim}{Dim}

 % "meq": must be equal
\newcommand{\meq}{\overset{!}{=}}

\newcommand{\R}{\mathbb{R}}
\newcommand*\Heq{\ensuremath{\overset{\kern2pt L'H}{=}}}
\usepackage{bm}
\newcommand{\uveci}{{\bm{\hat{\textnormal{\bfseries\i}}}}}
\newcommand{\uvecj}{{\bm{\hat{\textnormal{\bfseries\j}}}}}
\DeclareRobustCommand{\uvec}[1]{{%
  \ifcsname uvec#1\endcsname
     \csname uvec#1\endcsname
   \else
    \bm{\hat{\mathbf{#1}}}%
   \fi
}}
\usepackage[binary-units=true]{siunitx}

\makeatletter
\newcommand*{\balancecolsandclearpage}{%
  \close@column@grid
  \cleardoublepage
  \twocolumngrid
}
\makeatother

\newcounter{subproject}
\renewcommand{\thesubproject}{\alph{subproject}}
\newenvironment{subproj}{
\begin{description}
	\item[\refstepcounter{subproject}(\thesubproject)]
}{\end{description}}


\begin{document}
\title{Project 2 FYS3150}   % self-explanatory
\author{Eivind Støland, Anders P. Åsbø}               % self-explanatory
\date{\today}                             % self-explanatory
\noaffiliation                            % ignore this
\maketitle                                % creates the title, author, date


\tableofcontents

\section{Introduction} \label{sec:I}

\section{Formalism} \label{sec:II}

\subsection{Unitary transformations and eigenvalues} \label{sec:II:a}

We define a unitary transformation of a matrix \textbf{A} into a matrix \textbf{B} as:

\begin{align*}
\textbf{B} = \textbf{U}^T \textbf{AU} \, ,
\end{align*}

where \textbf{U} is a unitary matrix ($\textbf{U}^T\textbf{U} = \textbf{I}$, where \textbf{I} is the identity matrix). This type of transformation will preserve the eigenvalues of the system, meaning that \textbf{A} and \textbf{B} share the same eigenvalues. This can be seen by manipulating the eigenvalue equation for \textbf{A}:

\begin{align*}
\textbf{A}\textbf{v} = \lambda \textbf{v} \, , 
\end{align*} 

where $\lambda$ is the eigenvalue belonging to eigenvector \textbf{v}. We multiply by $\textbf{U}^T$ from the left and add the identity matrix in between \textbf{A} and \textbf{x}, and use that $\textbf{U}^T \textbf{U} = \textbf{UU}^T$:

\begin{align*}
\textbf{U}^T \textbf{AIv} &= \lambda \textbf{U}^T \textbf{v} \\
\textbf{U}^T \textbf{AU}^T \textbf{Uv} &= \lambda \textbf{U}^T \textbf{v} \\
(\textbf{U}^T \textbf{AU} ) (\textbf{U}^T \textbf{v}) &= \lambda (\textbf{U}^T \textbf{v}) \\
\textbf{Bw} &= \lambda \textbf{w} \, ,
\end{align*}

where we have defined $\textbf{w} = \textbf{U}^T \textbf{v}$. This is now an eigenvalue equation for \textbf{B} with $\lambda$ as the eigenvalue belonging to eigenvector \textbf{w}. This shows that the eigenvalues of the matrix is preserved through a unitary transformation.

A unitary transformation of on orthogonal set of vectors $\textbf{v}_i$ will conserve the orthogonality of the set of vectors. We show this by writing down the transformed set of vectors as:

\begin{align*}
\textbf{w}_i &= \textbf{U} \textbf{v}_i \, ,
\end{align*}

where $U$ is a unitary matrix. We need to check that $\textbf{w}_i^T \textbf{w}_j = \delta_{ij}$ for arbitrary $i$ and $j$:

\begin{align*}
\textbf{w}_i^T \textbf{w}_j &= (\textbf{U}\textbf{v}_i)^T (\textbf{U}\textbf{v}_j) \\
&= \textbf{v}_i^T \textbf{U}^T \textbf{U} \textbf{v}_j \\
&= \textbf{v}_i^T \textbf{v}_j \\
&= \delta_{ij} \, ,
\end{align*}

where we have used that $\textbf{U}$ is a unitary matrix and the orthogonality of the set of vectors $\vec{v}_i$. This shows that during a unitary transformation of a matrix the orthogonality of the eigenvectors is preserved. 


\subsection{Jacobi rotation method} \label{sec:II:b}

One method based on such unitary transformations is the Jacobi rotation method \citep{Jacobi1846}. We perform a series of unitary transformations of a matrix \textbf{A} until it is a diagonal matrix \textbf{D}:

\begin{align*}
\textbf{D} &= \textbf{U}_n^T ... \textbf{U}_1^T \textbf{AU}_1 ... \textbf{U}_n^T
\end{align*}  

As we have already seen that such transformations preserve the eigenvalues and the eigenvectors of \textbf{A}, we automatically have that the diagonal elements of \textbf{D} are the eigenvalues of \textbf{A}. As the eigenvectors of \textbf{D} are the standard basis vectors $\textbf{e}_i$, we can also use this to find the eigenvectors of \textbf{A}, $\textbf{v}_i$. In order to do this we note that:

\begin{align*}
\textbf{e}_i &= \textbf{U}_n^T ... \textbf{U}_1^T \textbf{v}_i \\
\textbf{U}_1 ... \textbf{U}_n \textbf{e}_i &= \textbf{U}_1 ... \textbf{U}_n \textbf{U}_n^T ... \textbf{U}_1^T \textbf{v}_i \\
\textbf{U}_1 ... \textbf{U}_n \textbf{e}_i &= \textbf{v}_i \, ,
\end{align*}

and in this way we can also find the eigenvectors of \textbf{A}. In general this is done as an iterative method were we get closer to a diagonal matrix with every iteration. The iteration is cut off once all the elements not on the diagonal are less than a specified tolerance. 

For the specifics of how this method is performed, including the unitary matrices used we refer to \citep{Hjorth-Jensen2015}. One important thing to note is that this method requires the matrix \textbf{A} to be symmetric. 

We will now present three problems based on one-dimensional wave equations. These can be recast into eigenvalue problems and in so doing we change them into something we can solve using the Jacobi rotation method. We can use the analytical solutions of these problems to verify the accuracy of the method.


\subsection{Buckling beam problem} \label{sec:II:c}

The buckling beam problem is one such problem. The equation is given by:

\begin{align*}
\gamma \frac{d^2 u }{dx^2} &= -Fu(x) \, ,
\end{align*}

where $u(x)$ is the displacement of the beam in the $y$ diretion, $F$ is a force applied at one of the ends of the beam, and $\gamma$ is a parameter defined by properties of the beam. We call the length of the beam $L$, which means that $x\in [0,L]$. We can set up a dimensionless variable $\rho = x/L$, where $\rho \in [0,1]$. This can be inserted into the equation:

\begin{align*}
\frac{\gamma}{L^2} \frac{d^2  }{d\rho^2} u(\rho) &= -Fu(\rho) \\
\frac{d^2}{d\rho^2} u(\rho) &= -\frac{FL^2}{\gamma} u(\rho)
\end{align*}

We define $\lambda = \frac{FL^2}{\gamma}$, which gives us the simplified equation:

\begin{align*}
- \frac{d^2}{d\rho^2} u(\rho) &= \lambda u (\rho)
\end{align*}

We apply Dirichlet boundary conditions $u(\rho_0) = u(\rho_\text{max}) = 0$ and discretize the equation by defining a set of points $\rho_i = \rho_0 + ih$, where $i = 1,2,...,N$ and $h = \frac{\rho_\text{max} - \rho_0}{N}$. We already know from the definition of $\rho$ that $\rho_0 = 0$ and $\rho_\text{max} = 1$. We can approximate the derivative in the equation:

\begin{align*}
\frac{d^2}{d\rho^2} u(\rho) &=  \frac{u(\rho+h) - 2u (\rho)  + u(\rho - h) }{h^2} + \mathcal{O}(h^2)  \, ,
\end{align*}

where we select $N$ so that $h$ is small enough such that we can neglect the higher order terms $\mathcal{O}(h^2)$. We use this approximation along with our discretization to rewrite the equation:

\begin{align*}
-\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} &= \lambda u_i \, ,
\end{align*} 

where $u_i = u(\rho_i)$. This can be rewritten as an eigenvalue problem:

\begin{align*}
\textbf{Au} = \lambda \textbf{u} \, ,
\end{align*}

where $\textbf{u}$ contains the $u_i$ with $i = 1,...,N-1$ (the boundary conditions fixes $u_0$ and $u_N$) and $\textbf{A}$ is a tridiagonal matrix with diagonal elements:

\begin{align*}
d = -\frac{2}{h^2}
\end{align*}

and elements in the bands above and below the diagonal:

\begin{align*}
e = \frac{1}{h^2}
\end{align*}

In other words the matrix takes the form:

\begin{align*}
\textbf{A} = \begin{bmatrix}
d & e & 0 & \cdots  & \cdots & \cdots & 0  \\
e & d & e & 0 & \cdots & 0 & 0 \\
0 & e & d & e & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \ddots & \ddots & \vdots &  \vdots \\
\vdots & \vdots & 0 & e & d & e & 0 \\
0 & \cdots & \cdots & 0 & e & d & e \\
0 & \cdots & \cdots & \cdots & 0 & e & d 
\end{bmatrix}
\end{align*}

Thus by finding the eigenvalues and eigenvectors of $\textbf{A}$ we can solve the buckling beam problem. The analytical eigenvalues of this matrix are:

\begin{align*}
\lambda_j &= d + 2a \cos ( \frac{j\pi}{N}) \quad , \quad j = 1,2,...,N-1 \, ,
\end{align*}

and the eigenvectors are:

\begin{align*}
\textbf{u}_j &= \begin{bmatrix}
\sin( \frac{j\pi}{N} ) \\
\sin( \frac{2j\pi}{N} ) \\
\vdots \\
\sin( \frac{(N-1)j\pi}{N} ) 
\end{bmatrix} \quad , \quad j = 1,2,...,N-1
\end{align*}

These can be compared with our numerical results. The derivation of this analytical solution is detailed in \citep{Lyche2017}. 



\subsection{Quantum mechanical harmonic oscillator with one electron as an eigenvalue problem} \label{sec:II:d}

The harmonic oscillator potential is central symmetric. This means that the Schrödinger equation can be split into a radial part and an angular part, where the angular part has an analytic solution. We ignore this part of the equation and instead focus only on the radial part:

\begin{align*}
-\frac{\hbar^2}{2m} \bigg( \frac{1}{r^2} \frac{d}{dr}r^2 \frac{d}{dr} - \frac{l(l+1)}{r^2} \bigg) R(r) + \frac{1}{2}kr^2 R(r) = ER(r) \, ,
\end{align*} 

where $k = m\omega^2$ with $\omega$ being the oscillator frequency and $m$ being the mass. $r$ is the radial coordinate, $l$ is the angular momentum quantum number, $R(r)$ is the radial part of the wave equation, and $E$ is the energy. We can substitute $R(r) = u(r)/r$ which simplifies the equation:

\begin{align*}
-\frac{\hbar^2}{2m} \frac{d^2}{dr^2} u(r) + \bigg( \frac{1}{2}kr^2 + \frac{l(l+1)}{r^2} \frac{\hbar^2}{2m} \bigg) u(r) &= Eu(r) \, ,
\end{align*}

where we have separated the differential from the rest of the equation as well, which will be useful later on. To further simplify we can scale the equation. We do this by introducing a dimensionless variable $\rho = r/\alpha$, where $\alpha$ is a constant. Inserting this gives:

\begin{align*}
-\frac{\hbar^2}{2m\alpha^2} \frac{d^2}{d\rho^2} u(\rho) + \bigg( \frac{1}{2}k\alpha^2 \rho^2 + \frac{l(l+1)}{\rho^2} \frac{\hbar^2}{2m\alpha^2} \bigg) u(\rho) &= Eu(\rho) 
\end{align*}

From here on out we will assume the system to be in the lowest orbital state ($l=0$), which simplifies the equation above furter:

\begin{align*}
-\frac{\hbar^2}{2m\alpha^2} \frac{d^2}{d\rho^2} u(\rho) + \frac{1}{2}k\alpha^2 \rho^2 u(\rho) &= Eu(\rho) 
\end{align*}

We multiply by $\frac{2m\alpha^2}{\hbar^2}$ on both sides:

\begin{align*}
-\frac{d^2}{d\rho^2} u(\rho) + \frac{mk}{\hbar^2}\alpha^4 \rho^2 u(\rho) &= \frac{2m\alpha^2}{\hbar^2} Eu(\rho) 
\end{align*}

As $\alpha$ is an arbitrary constant we can choose $\alpha = \sqrt{\frac{\hbar}{m \omega}}$, and define $\lambda = \frac{2m\alpha^2}{\hbar^2} E = \frac{2}{\hbar \omega} E$:

\begin{align*}
-\frac{d^2}{d\rho^2} u(\rho) + \rho^2 u(\rho) &= \lambda u(\rho) 
\end{align*}

We know that the allowed energy levels for a quantum harmonic oscillator are $E_n = \hbar \omega (n + \frac{1}{2})$, which gives us that $\lambda_n = 2n+1$. This can be compared with a numerical calculation later to check the accuracy of our results.
 
In general we have that $u(0) = u(\infty) = 0$. This is not feasible on a computer and as such we define a $\rho_{\text{max}}$ and set boundary conditions $u(0) = u(\rho_{\text{max}}) = 0$. We then define a set of points $\rho_i = \rho_0 + ih$ where $i = 1,2,...,N$ and $h = \frac{\rho_N - \rho_0}{N}$, and set $\rho_0 = 0$ and $\rho_N  = \rho_{\text{max}}$. These points can be used to discretize the previous equation (approximating the derivative as well):

\begin{align*}
\frac{-u_{i+1} + 2u_i - u_{i-1}}{h^2} + \rho_i^2 u_i = \lambda u_i \, ,
\end{align*}

where $u_i = u(\rho_i)$. This can be set up as an eigenvalue problem:

\begin{align*}
\textbf{Au} = \lambda \textbf{u} \, ,
\end{align*}

where $\textbf{u}$ contains all the $u_i$. The matrix \textbf{A} will have diagonal elements:

\begin{align*}
d_i = \frac{2}{h^2} + \rho_i^2 \, ,
\end{align*}

and elements above and below the diagonal:

\begin{align*}
e = -\frac{1}{h^2}
\end{align*}

We can now use numerical methods such as the Jacobi rotation algorithm to find the eigenvalues and eigenvectors (solutions) of \textbf{A} as it is a symmetrical matrix.


\subsection{Quantum mechanical harmonic oscillator with two electrons as an eigenvalue problem} \label{sec:II:e}
 
Similarly to how we did in the previous section we can find a numerical solution to the Schrödinger equation where we have two electrons in a harmonic oscillator instead of one. As the electrons can interact we now need to add the Coulomb potential to the equation. We ignore this for now, but will add it later. The radial part of the Schrödinger equation with two electrons in a harmonic oscillator is (where some of the steps from the previous section has already been applied):

\begin{align*}
\bigg( -\frac{\hbar^2}{2m} \frac{d^2}{dr_1^2} - \frac{\hbar^2}{2m} \frac{d^2}{dr_2^2} + \frac{1}{2}kr_1^2 + \frac{1}{2}kr_2^2 \bigg) u(r_1,r_2) =  Eu(r_1,r_2) \, ,
\end{align*}

where $E$ now is the total energy of the system. Inserting the relative coordinate $\textbf{r} = \textbf{r}_1 - \textbf{r}_2$ and the center-of-mass coordinate $\textbf{R} = \frac{\textbf{r}_1 + \textbf{r}_2}{2}$ gives us:

\begin{align*}
\bigg( -\frac{\hbar^2}{m} \frac{d^2}{dr^2} - \frac{\hbar^2}{4m} \frac{d^2}{dR^2} + \frac{1}{4}kr^2 + kR^2\bigg) u(r,R) &= Eu(r,R)
\end{align*}

This can be separated again into an $R$-dependent and an $r$-dependent part by setting $u(r,R) = \psi(r) \phi(R)$ and separating the energy into $E = E_r + E_R$. The $R$-dependent part is essentially the same problem we discussed in the previous section and so we discuss only the $r$-dependent part from here on out.

The Coulomb potential is given by $V(r) = \frac{\beta e^2}{r}$, where $\beta e^2 = 1.44$ eV nm. We add this to the equation:

\begin{align*}
\bigg( - \frac{\hbar^2}{m} \frac{d^2}{dr^2} + \frac{1}{4}kr^2 + \frac{\beta e^2}{r} \bigg) \psi(r) = E_r \psi(r)
\end{align*}

Again we introduce the dimensionless variable $\rho = r/\alpha$ and multiply by $\frac{m\alpha^2}{\hbar^2}$ on both sides of the equation:

\begin{align*}
\bigg( - \frac{d^2}{d\rho^2} + \frac{1}{4} \frac{mk}{\hbar^2} \alpha^4 \rho^2 + \frac{m\alpha \beta e^2}{\rho \hbar^2} \bigg) \psi (\rho) &= \frac{m\alpha^2}{\hbar^2} E_r \psi(\rho)
\end{align*} 

We define $\omega_r^2 = \frac{1}{4} \frac{mk}{\hbar^2} \alpha^4$ and $\lambda = \frac{m\alpha^2}{\hbar^2}E_r$, and set $\alpha = \frac{\hbar^2}{m\beta e^2}$. The equation can then be rewritten into:

\begin{align*}
- \frac{d^2}{d\rho^2} \psi(\rho) + \omega_r^2 \rho^2 \psi(\rho) + \frac{1}{\rho} \psi(\rho) = \lambda \psi(\rho)
\end{align*}

This equation has analytical solutions (detailed in \citep{PhysRevA.48.3561}) for $\lambda$:

\begin{align*}
\lambda_i &= 3 \omega_r^{3/2} +\sqrt{3} \omega_r (2i + 1)
\end{align*}

The boundary conditions will be the same as in the previous section, and we define the same discretization. This gives us:

\begin{align*}
\frac{-u_{i+1} + 2u_i - u_{i-1}}{h^2} + \bigg( \omega_r^2\rho_i^2 + \frac{1}{\rho_i} \bigg) u_i = \lambda u_i 
\end{align*}

This can be set up as an eigenvalue problem in the same way as in the previous section:

\begin{align*}
\textbf{Au} = \lambda \textbf{u} \, ,
\end{align*}

where the diagonal elements in \textbf{A} are given by:

\begin{align*}
d_i &= \frac{2}{h^2} + \omega_r^2 \rho_i^2 + \frac{1}{\rho_i}
\end{align*}

and the elements above and below the diagonal are:

\begin{align*}
e &= - \frac{1}{h^2}
\end{align*}

This can similarly be solved using the Jacobi rotation method as the matrix is symmetric.

\newpage


\section{Method} \label{sec:III}

\subsection{Implementation of Jacobi rotation algorithm} \label{III:a}

We have implemented the Jacobi rotation algorithm as a class in C++. The code is linked to in \hyperref[A]{appendix A}. It is instantiated with two Armadillo \citep{Armadillo} matrices \textbf{A} and \textbf{R} and the dimensionality of the matrices as an integer \verb+N+. It is worth noting that this is not the same $N$ defined in the \hyperref[sec:II]{section II}, as the dimensionality of the matrices is one less than $N$. At input the matrix \textbf{A} is the matrix for which we want to calculate eigenvalues and eigenvectors and \textbf{R} should be the identity matrix, but can be any matrix with the same dimensionality as \textbf{A} as the class sets it up correctly anyway. Running the \verb+solve()+ method belonging to the class will replace the elements in \textbf{A} and \textbf{R} such that the diagonal elements of \textbf{A} will be the eigenvalues and the rows of \textbf{R} will be the eigenvectors. 

The \verb+solve()+ method performs the following while-loop:

\begin{cpp}
double epsilon = 1.0e-8; 
double max_number_iterations = double(N)*double(N)*double(N);
int iterations = 0;
double max_off_diag = max_offdiag();
while ( fabs(max_off_diag)>epsilon &&
        double(iterations)<max_number_iterations){
    max_off_diag = max_offdiag();
    rotate();
    iterations++;
}
\end{cpp}

A tolerance \verb+epsilon+ is set, along with a cap on the number of iterations, \verb+max_number_iterations+. For every iteration it checks whether the pivot element (the largest element not on the diagonal) is larger than the tolerance and that the cap on iterations has not been reached yet. In every iteration we get the pivot element using \verb+max_offdiag()+ and the \verb+rotate()+ function, which performs one transformation of the matrix, is called. 

The \verb+max_offdiag()+ function works as follows:

\begin{cpp}
double max = 0.0;
for(int i = 0; i < N; i++ ){
  for(int j = i + 1; j < N; j++ ){
    if( fabs((*A)(i,j)) > max ){
      max = fabs((*A)(i,j));
      l = i;
      k = j;
    }
  }
}
\end{cpp}

It simply checks all the elements in the upper triangular part of the matrix and finds the largest element. The largest element is stored in \verb+max+ and is returned by the function. The indices of the element is also stored in the integers \verb+k+ and \verb+l+ which are stored internally in the class.

The \verb+rotate()+ function works as follows:

\begin{cpp}
double s,c;
if ((*A)(k,l) != 0.0){
  double t,tau;
  tau = ((*A)(l,l)- (*A)(k,k))/(2*(*A)(k,l));
  if (tau > 0){
    t = 1.0/(tau + sqrt(1.0 + tau*tau));
  }
  else {
    t = -1.0/(-tau + sqrt(1.0 + tau*tau));
  }
  c = 1.0/sqrt(1+t*t);
  s = c*t;
}
else {
  c = 1.0;
  s = 0.0;
}

double a_kk, a_ll, a_ik, a_il, r_ik, r_il;
a_kk = (*A)(k,k);
a_ll = (*A)(l,l);

(*A)(k,k) = c*c*a_kk - 2.0*c*s*(*A)(k,l) + s*s*a_ll;
(*A)(l,l) = s*s*a_kk + 2.0*c*s*(*A)(k,l) + c*c*a_ll;
(*A)(k,l) = 0.0; // hard-coding of the zeros
(*A)(l,k) = 0.0;

for (int i = 0; i<N; ++i){
  if (i != k && i != l) {
    a_ik = (*A)(i,k);
    a_il = (*A)(i,l);
    (*A)(i,k) = c*a_ik - s*a_il;
    (*A)(k,i) = (*A)(i,k);
    (*A)(i,l) = c*a_il + s*a_ik;
    (*A)(l,i) = (*A)(i,l);
  }

  r_ik = (*R)(i,k);
  r_il = (*R)(i,l);
  (*R)(i,k) = c*r_ik - s*r_il;
  (*R)(i,l) = c*r_il + s*r_ik;
}
\end{cpp}

First it calculates the variables \verb+s+ and \verb+c+. These variables are as outlined in \citep{Hjorth-Jensen2015}. Then it performs the rotation by calculating the new elements of \verb+A+ and \verb+R+ and replacing the old ones.

\subsubsection{Computational cost} \label{III:a:i}

We are interested in how fast the solution converges to the correct one. To this end we define:

\begin{align*}
\text{off}(\textbf{A})^2 &= \sum\limits_{ij} |a_{ij}|^2 \, ,
\end{align*}

where $a_{ij}$ are the components of matrix \textbf{A}. We define the pivot element $p$ as the largest element in the matrix:

\begin{align*}
|p| = |\text{max}(a_{ij})|
\end{align*}

There are $N(N-1)$ elements not on the diagonal in the matrix, which means that at all times we have:

\begin{align*}
\text{off}(\textbf{A})^2 \leq N(N-1)|p|^2 \\
\frac{\text{off}(\textbf{A})^2}{N(N-1)} \leq |p|^2
\end{align*}

For each rotation we reduce $\text{off}(\textbf{A})^2$ by $2|p|^2$, meaning that after a rotation we have (noting the amount of iterations the matrix has been through by a superscript):

\begin{align*}
\text{off}(\textbf{A}^{(1)})^2 = \text{off}(\textbf{A})^2 - 2|p|^2 \, ,
\end{align*}

By using the earlier inequality this gives us:

\begin{align*}
\text{off}(\textbf{A}^{(1)})^2 \leq (1 - \frac{2}{N(N-1)}) \text{off}(\textbf{A})^2 \\
\text{off}(\textbf{A}^{(1)}) \leq \sqrt{1 - \frac{2}{N(N-1)}} \text{off}(\textbf{A}) 
\end{align*}

This means that:

\begin{align*}
\text{off}(\textbf{A}^{(k)}) \leq \bigg(1 - \frac{2}{N(N-1)}\bigg)^{k/2} \text{off}(\textbf{A}) 
\end{align*}

As long as $N$ is large we can approximate:

\begin{align*}
1 - \frac{2}{N(N-1)} \approx e^{-\frac{2}{N(N-1)}}
\end{align*}

After $k$ iterations we then have the approximate relation:

\begin{align*}
\text{off}(\textbf{A}^{(k)}) \lesssim e^{-\frac{k}{N(N-1)}} \text{off}(\textbf{A}) 
\end{align*} 

This means that the convergence of the solution is dependent on several things. It is obviously dependent on the amount of iterations $k$, but it is also dependent on $\text{off}(\textbf{A})$ and $N$. If we assume $\text{off}(\textbf{A})$ to be something of reasonable size we have that the factor $e^{-\frac{k}{N(N-1)}}$ needs to be small. In order for this to be small we need $\frac{k}{N(N-1)}$ to be large, and for this to be larger than 1 even we need at least $k \geq N(N-1) \approx N^2$. Thus, as we assume $N$ to be a large number, $k = N^3$ would result in $e^{-\frac{k}{N(N-1)}}$ being a small number, and that is what we have set as a cap on the maximum amount of iterations the solver performs. It is worth noting that what we derived above is an approximation to the \textit{slowest rate possible} that the method can converge. Usually the solution converges faster than this.


\subsubsection{Error sources} \label{III:a:ii}


\subsection{Error sources when solving quantum mechanical problems} \label{III:b} 

\section{Results} \label{sec:IV}

\section{Discussion} \label{sec:V} 

\section{Conclusion} \label{sec:VI}


\bibliography{kilder.bib}{}

\appendix
\section{Project files} \label{A}

\end{document}